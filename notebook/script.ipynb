{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd89e64b",
   "metadata": {},
   "source": [
    "# Goal\n",
    "    1.Data Ingestion - Load PDFs, text files, HTML, CSVs\n",
    "    2.Advanced Chunking - Recursive, semantic\n",
    "    3.Vector Indexing - ChromaDB\n",
    "    4.Hybrid Search - Dense (embeddings) + Sparse (BM25)\n",
    "    5.Re-ranking -Cross-Encoder models\n",
    "    6.Query Transformation - Multi-query, HyDE, Step-back prompting\n",
    "    7.Context Compression - LLM-based relevance filtering\n",
    "    8.Generation with Citations - Answers with source attribution\n",
    "    9.Evaluation Metrics - MRR, Recall@K, answer quality\n",
    "    10.Complete Orchestration - Easy-to-use pipeline class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db467b2",
   "metadata": {},
   "source": [
    "## 1.Data Ingestion - Load PDFs, text files, HTML, CSVs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630c951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e8389f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nfrom langchain.document_loaders import (PyPDFLoader,TextLoader,Docx2txtLoader,DirectoryLoader,UnstructuredHTMLLoader,CSVLoader)\\nfrom typing import List,Dict,Tuple\\nimport re\\n\\nclass DataIngestion:\\n\\n    @staticmethod\\n    def load_pdfs(file_path:str):\\n        loader=PyPDFLoader(file_path)\\n        return loader.load()\\n\\n    @staticmethod\\n    def load_text(file_path:str):\\n        loader=TextLoader(file_path)\\n        return loader.load()\\n\\n    @staticmethod\\n    def load_directory(directory_path:str,glob_pattern:str=\\'**/*.pdf\\'):\\n        loader=DirectoryLoader(\\n            directory_path,\\n            glob=glob_pattern,\\n            loader_cls=PyPDFLoader,\\n            show_progress=True\\n        )\\n        return loader.load()\\n\\n    @staticmethod\\n    def load_docx(file_path:str):\\n        loader=Docx2txtLoader(file_path)\\n        return loader.load()\\n\\n    @staticmethod\\n    def preprocess_text(text:str)->str:\\n        text=re.sub(r\"\\\\s+\",\\' \\',text)\\n        text=re.sub(r\\'[^\\\\w\\\\s\\\\.\\\\?\\\\!\\\\-\\\\:\\\\;]\\',\\'\\',text)\\n\\n        return text.strip()\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "from langchain.document_loaders import (PyPDFLoader,TextLoader,Docx2txtLoader,DirectoryLoader,UnstructuredHTMLLoader,CSVLoader)\n",
    "from typing import List,Dict,Tuple\n",
    "import re\n",
    "\n",
    "class DataIngestion:\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdfs(file_path:str):\n",
    "        loader=PyPDFLoader(file_path)\n",
    "        return loader.load()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_text(file_path:str):\n",
    "        loader=TextLoader(file_path)\n",
    "        return loader.load()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_directory(directory_path:str,glob_pattern:str='**/*.pdf'):\n",
    "        loader=DirectoryLoader(\n",
    "            directory_path,\n",
    "            glob=glob_pattern,\n",
    "            loader_cls=PyPDFLoader,\n",
    "            show_progress=True\n",
    "        )\n",
    "        return loader.load()\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_docx(file_path:str):\n",
    "        loader=Docx2txtLoader(file_path)\n",
    "        return loader.load()\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_text(text:str)->str:\n",
    "        text=re.sub(r\"\\s+\",' ',text)\n",
    "        text=re.sub(r'[^\\w\\s\\.\\?\\!\\-\\:\\;]','',text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff72b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document=DataIngestion.load_directory(r'C:\\Users\\evilk\\OneDrive\\Desktop\\Projects\\RAG-Complete-Pipeline\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37738179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(document))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82d2ce",
   "metadata": {},
   "source": [
    "## 2.Advanced Chunking - Recursive, semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4444ecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import (RecursiveCharacterTextSplitter,CharacterTextSplitter)\\nfrom sentence_transformers import SentenceTransformer\\nfrom langchain.schema import Document\\nimport numpy as np\\n\\nclass Chunking:\\n\\n\\n    @staticmethod\\n    def recursive_chunking(documents,chunk_size=1000,chunk_overlap=200):\\n        textSplitter=RecursiveCharacterTextSplitter(\\n            chunk_size=chunk_size,\\n            chunk_overlap=chunk_overlap,\\n            length_function=len,\\n            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\\n        )\\n        return textSplitter.split_documents(documents)\\n\\n    @staticmethod\\n    def semantic_chunking(documents,embedding,chunk_size=1000):\\n         chunks=[]\\n         model=SentenceTransformer(\\'sentence-transformers/all-MiniLM-L6-v2\\')\\n\\n         for doc in documents:\\n             sentences=re.split(r\\'(?<=[.!?])\\\\s+\\',doc.page_content)\\n\\n             if len(sentences)<=1:\\n                 chunks.append(doc)\\n                 continue\\n\\n             embedding_array=model.encode(sentences)\\n\\n             similarities=[]\\n             for i in range(len(embedding_array)-1):\\n                 sim=np.dot(embedding_array[i],embedding_array[i+1])\\n                 similarities.append(sim)\\n\\n             threshold=np.percentile(similarities,30)\\n\\n             current_chunk=[]\\n             for i,sentence in enumerate(sentences):\\n                 current_chunk.append(sentence)\\n\\n                 if i <len(similarities) and similarities[i]<threshold:\\n                     chunk_text=\\' \\'.join(current_chunk)\\n                     if len(chunk_text)>chunk_size:\\n                         chunks.append(Document(\\n                             page_content=chunk_text,\\n                             metadata=doc.metadata\\n                         ))\\n                         current_chunk=[]\\n\\n             if current_chunk:\\n                chunks.append(Document(\\n                    page_content=\\' \\'.join(current_chunk),\\n                    metadata=doc.metadata\\n                ))\\n         return chunks\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from langchain.text_splitter import (RecursiveCharacterTextSplitter,CharacterTextSplitter)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "\n",
    "class Chunking:\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def recursive_chunking(documents,chunk_size=1000,chunk_overlap=200):\n",
    "        textSplitter=RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        return textSplitter.split_documents(documents)\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_chunking(documents,embedding,chunk_size=1000):\n",
    "         chunks=[]\n",
    "         model=SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "         \n",
    "         for doc in documents:\n",
    "             sentences=re.split(r'(?<=[.!?])\\s+',doc.page_content)\n",
    "             \n",
    "             if len(sentences)<=1:\n",
    "                 chunks.append(doc)\n",
    "                 continue\n",
    "             \n",
    "             embedding_array=model.encode(sentences)\n",
    "             \n",
    "             similarities=[]\n",
    "             for i in range(len(embedding_array)-1):\n",
    "                 sim=np.dot(embedding_array[i],embedding_array[i+1])\n",
    "                 similarities.append(sim)\n",
    "                 \n",
    "             threshold=np.percentile(similarities,30)\n",
    "             \n",
    "             current_chunk=[]\n",
    "             for i,sentence in enumerate(sentences):\n",
    "                 current_chunk.append(sentence)\n",
    "                 \n",
    "                 if i <len(similarities) and similarities[i]<threshold:\n",
    "                     chunk_text=' '.join(current_chunk)\n",
    "                     if len(chunk_text)>chunk_size:\n",
    "                         chunks.append(Document(\n",
    "                             page_content=chunk_text,\n",
    "                             metadata=doc.metadata\n",
    "                         ))\n",
    "                         current_chunk=[]\n",
    "                         \n",
    "             if current_chunk:\n",
    "                chunks.append(Document(\n",
    "                    page_content=' '.join(current_chunk),\n",
    "                    metadata=doc.metadata\n",
    "                ))\n",
    "         return chunks\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce27889b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nchunks=Chunking.recursive_chunking(document)\\nprint(chunks[1000])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "chunks=Chunking.recursive_chunking(document)\n",
    "print(chunks[1000])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319cf81",
   "metadata": {},
   "source": [
    "## 3.Vector Indexing - ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac57bb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sentence_transformers import SentenceTransformer\\nfrom langchain.vectorstores import FAISS,Chroma\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\nimport numpy as np\\n\\nclass Embeddings:\\n\\n    def __init__(self,model_name=\\'all-MiniLM-L6-v2\\'):\\n\\n        self.embeddings=HuggingFaceEmbeddings(\\n            model_name=f\"sentence-transformers/{model_name}\",\\n            model_kwargs={\\'device\\':\\'cuda\\'},\\n            encode_kwargs={\\'normalize_embeddings\\':True}\\n        )\\n\\n    def create_chroma_db(self,chunks,persist_directory=\"../chroma_db\"):\\n        vectordb=Chroma.from_documents(\\n            documents=chunks,\\n            embedding=self.embeddings,\\n            persist_directory=persist_directory\\n        )\\n        return vectordb\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import FAISS,Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "class Embeddings:\n",
    "    \n",
    "    def __init__(self,model_name='all-MiniLM-L6-v2'):\n",
    "        \n",
    "        self.embeddings=HuggingFaceEmbeddings(\n",
    "            model_name=f\"sentence-transformers/{model_name}\",\n",
    "            model_kwargs={'device':'cuda'},\n",
    "            encode_kwargs={'normalize_embeddings':True}\n",
    "        )\n",
    "        \n",
    "    def create_chroma_db(self,chunks,persist_directory=\"../chroma_db\"):\n",
    "        vectordb=Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        return vectordb\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a61f0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nemb=Embeddings()\\nvectordb=emb.create_chroma_db(chunks)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "emb=Embeddings()\n",
    "vectordb=emb.create_chroma_db(chunks)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3a676",
   "metadata": {},
   "source": [
    "## 4.Hybrid Search - Dense (embeddings) + Sparse (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc981e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom rank_bm25 import BM25Okapi\\nimport numpy as np\\n\\nclass HybridRetriever:\\n\\n    def __init__(self,vectorstore,documents):\\n        self.vectorstore=vectorstore\\n        self.documents=documents\\n\\n        tokenized_docs=[doc.page_content.lower().split() for doc in documents]\\n        self.bm25=BM25Okapi(tokenized_docs)\\n        print(f\"Hybrid Retriever ready with {len(documents)} documents\")\\n\\n\\n    def retrieve(self,query:str,k=10,alpha=0.5):\\n\\n        # Vector Search \\n        dense_results=self.vectorstore.similarity_search_with_score(query,k=k*2)\\n\\n        # BM25 Search\\n        tokenized_query=query.lower().split()\\n        bm25_scores=self.bm25.get_scores(tokenized_query)\\n\\n        #Normalized Scores between 0-1 \\n        dense_scores=np.array([1/(1+score) for _,score in dense_results])\\n        if dense_scores.max()>dense_scores.min():\\n            dense_scores=(dense_scores-dense_scores.min())/(dense_scores.max()-dense_scores.min())\\n\\n        if bm25_scores.max()>bm25_scores.min():\\n            bm25_scores=(bm25_scores-bm25_scores.min())/(bm25_scores.max()-bm25_scores.min())\\n\\n        doc_scores={}\\n        # ADD dense scores\\n        for i, (doc, _) in enumerate(dense_results):\\n            doc_id=id(doc)\\n            doc_scores[doc_id]={\\'doc\\':doc,\\'score\\':alpha*dense_scores[i]}\\n\\n        #ADD Sparse scores\\n        for i,doc in enumerate(self.documents):\\n            doc_id=id(doc)\\n            if doc_id in doc_scores:\\n                doc_scores[doc_id][\\'score\\']+=(1-alpha)*bm25_scores[i]\\n            else:\\n                doc_scores[doc_id]={\\'doc\\':doc,\\'score\\':(1-alpha)*bm25_scores[i]}\\n\\n        # Sort by combined score \\n        sorted_docs=sorted(doc_scores.values(),key=lambda x:x[\\'score\\'],reverse=True)[:k]\\n\\n        return[(item[\\'doc\\'],item[\\'score\\']) for item in sorted_docs]\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "class HybridRetriever:\n",
    "    \n",
    "    def __init__(self,vectorstore,documents):\n",
    "        self.vectorstore=vectorstore\n",
    "        self.documents=documents\n",
    "        \n",
    "        tokenized_docs=[doc.page_content.lower().split() for doc in documents]\n",
    "        self.bm25=BM25Okapi(tokenized_docs)\n",
    "        print(f\"Hybrid Retriever ready with {len(documents)} documents\")\n",
    "        \n",
    "        \n",
    "    def retrieve(self,query:str,k=10,alpha=0.5):\n",
    "        \n",
    "        # Vector Search \n",
    "        dense_results=self.vectorstore.similarity_search_with_score(query,k=k*2)\n",
    "        \n",
    "        # BM25 Search\n",
    "        tokenized_query=query.lower().split()\n",
    "        bm25_scores=self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        #Normalized Scores between 0-1 \n",
    "        dense_scores=np.array([1/(1+score) for _,score in dense_results])\n",
    "        if dense_scores.max()>dense_scores.min():\n",
    "            dense_scores=(dense_scores-dense_scores.min())/(dense_scores.max()-dense_scores.min())\n",
    "            \n",
    "        if bm25_scores.max()>bm25_scores.min():\n",
    "            bm25_scores=(bm25_scores-bm25_scores.min())/(bm25_scores.max()-bm25_scores.min())\n",
    "            \n",
    "        doc_scores={}\n",
    "        # ADD dense scores\n",
    "        for i, (doc, _) in enumerate(dense_results):\n",
    "            doc_id=id(doc)\n",
    "            doc_scores[doc_id]={'doc':doc,'score':alpha*dense_scores[i]}\n",
    "            \n",
    "        #ADD Sparse scores\n",
    "        for i,doc in enumerate(self.documents):\n",
    "            doc_id=id(doc)\n",
    "            if doc_id in doc_scores:\n",
    "                doc_scores[doc_id]['score']+=(1-alpha)*bm25_scores[i]\n",
    "            else:\n",
    "                doc_scores[doc_id]={'doc':doc,'score':(1-alpha)*bm25_scores[i]}\n",
    "                \n",
    "        # Sort by combined score \n",
    "        sorted_docs=sorted(doc_scores.values(),key=lambda x:x['score'],reverse=True)[:k]\n",
    "        \n",
    "        return[(item['doc'],item['score']) for item in sorted_docs]\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb7520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nhybrid = HybridRetriever(vectorstore=vectordb, documents=chunks)\\n\\nquery = \"company leave policy for new employees\"\\nresults = hybrid.retrieve(query, k=5, alpha=0.6)\\n\\nfor doc, score in results:\\n    print(score,\" \",doc.page_content[:100])\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hybrid = HybridRetriever(vectorstore=vectordb, documents=chunks)\n",
    "\n",
    "query = \"company leave policy for new employees\"\n",
    "results = hybrid.retrieve(query, k=5, alpha=0.6)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(score,\" \",doc.page_content[:100])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c13658",
   "metadata": {},
   "source": [
    "##  5.Re-ranking -Cross-Encoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1025 00:20:30.908000 6136 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List\n",
    "class ReRanker:\n",
    "    \n",
    "    def __init__(self,model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        \n",
    "        print(f\"Loading re-ranker model: {model_name}...\")\n",
    "        self.model=CrossEncoder(model_name)\n",
    "        print(\"Re-Ranker loaded\")\n",
    "        \n",
    "    def rerank(self,query:str,documents:List,top_n=5):\n",
    "        \n",
    "        pairs=[[query,doc.page_content] for doc in documents]\n",
    "        \n",
    "        scores=self.model.predict(pairs)\n",
    "        \n",
    "        scored_docs=list(zip(documents,scores))\n",
    "        scored_docs.sort(key=lambda x : x[1],reverse=True)\n",
    "        \n",
    "        return [doc for doc,_ in scored_docs[:top_n]]\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60bafc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquery = \"company leave policy for new employees\"\\nreranker=ReRanker()\\ntop_docs=reranker.rerank(query,document,top_n=5)\\nprint(top_docs[0])\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "query = \"company leave policy for new employees\"\n",
    "reranker=ReRanker()\n",
    "top_docs=reranker.rerank(query,document,top_n=5)\n",
    "print(top_docs[0])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6244e",
   "metadata": {},
   "source": [
    "## 6.Query Transformation - Multi-query, HyDE, Step-back prompting (API call HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d9a6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "class QueryTransformer:\n",
    "    \n",
    "    def __init__(self,model_name=\"meta-llama/Llama-3.1-8B\"):\n",
    "        \n",
    "        load_dotenv()\n",
    "        hf_token=os.getenv(\"HF_TOKEN\")\n",
    "        \n",
    "        if not hf_token:\n",
    "            raise ValueError(\"HF_TOKEN is not found!!!!\")\n",
    "        \n",
    "        self.client=HuggingFaceEndpoint(\n",
    "            repo_id=model_name,\n",
    "            task=\"text-generation\",\n",
    "            huggingfacehub_api_token=hf_token\n",
    "        )\n",
    "        self.model_name=model_name\n",
    "        print(f\"Using HF model : {model_name}\")\n",
    "        \n",
    "    def _generate(self,prompt:str,max_new_token=256):\n",
    "        \n",
    "        reponse=self.client.invoke(prompt,max_new_tokens=max_new_token)\n",
    "        return reponse.strip()\n",
    "    \n",
    "    def multi_query(self,original_query:str,num_queries=3):\n",
    "        prompt=f\"\"\"Generate {num_queries} different versions of this question to retrieve relevant documents.\n",
    "        Only output the questions, one per line,without numbering.\n",
    "        Original question :{original_query}\n",
    "        Alternative question :\"\"\"\n",
    "        \n",
    "        response = self._generate(prompt)\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        queries=[original_query]+queries[:num_queries]\n",
    "        return queries\n",
    "    \n",
    "    def hyde(self,query:str):\n",
    "        prompt=f\"\"\"Write a detailed,factual answer to this question:\n",
    "        Question : {query}\n",
    "        Answer :\"\"\"\n",
    "        return self._generate(prompt)\n",
    "    \n",
    "    \n",
    "    def step_back(self,query:str):\n",
    "        prompt=f\"\"\"Give this specific question , generate a broader,more general question:\n",
    "        Specific question :{query}\n",
    "        Broader question :\"\"\"\n",
    "        \n",
    "        return self._generate(prompt)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe980e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HF model : meta-llama/Llama-3.1-8B\n",
      "Multi Queries: ['What is LangChain?', 'What is LNC?', 'Alternative question :What is LNC?', 'Alternative question :What is LNC?']\n",
      "Hypothetical answer: LangChain is a Python-based framework that allows developers to build complex language models and AI-powered applications. It provides a set of APIs and modules that enable developers to create models that can generate and manipulate text, answer questions, and perform other natural language processing (NLP) tasks.\n",
      "        The framework was developed by a team of researchers and engineers at OpenAI, and it is built on top of the OpenAI API, a powerful tool for building AI-powered applications. LangChain also includes a set of pretrained models and datasets, which can be used to train and fine-tune the models.\n",
      "        Some of the key features of LangChain include:\n",
      "        Language Modeling: LangChain includes a set of pretrained language models, such as BERT, GPT-3, and XLNet, which can be used to generate text and perform other NLP tasks.\n",
      "        Question Answering: LangChain includes a question answering module that allows developers to build applications that can answer questions using the language models.\n",
      "        Data Processing: LangChain includes a set of data processing modules that allow developers to preprocess and prepare data for training and testing.\n",
      "        Model Fine-tuning: LangChain includes a module for fine-tuning language models, which allows developers to customize the models to their specific use case.\n"
     ]
    }
   ],
   "source": [
    "qt = QueryTransformer(model_name=\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "queries = qt.multi_query(\"What is LangChain?\")\n",
    "print(\"Multi Queries:\", queries)\n",
    "\n",
    "answer = qt.hyde(\"What is LangChain?\")\n",
    "print(\"Hypothetical answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2f778",
   "metadata": {},
   "source": [
    "## 6.Query Transformation - Multi-query, HyDE, Step-back prompting (Local Ollama model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "class LocalQueryTransformer:\n",
    "    \n",
    "    \n",
    "    def __init__(self, model=\"llama3.2\"):\n",
    "\n",
    "        self.llm = Ollama(model=model, temperature=0)\n",
    "        print(f\"Using Ollama model: {model}\")\n",
    "    \n",
    "    def multi_query(self, original_query: str, num_queries=3):\n",
    "        \n",
    "        prompt = f\"\"\"Generate {num_queries} different versions of this question to retrieve relevant documents.\n",
    "                Only output the questions, one per line, without numbering.\n",
    "\n",
    "                Original question: {original_query}\n",
    "\n",
    "                Alternative questions:\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "     \n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip() and '?' in q]\n",
    "        queries = [original_query] + queries[:num_queries]\n",
    "        \n",
    "        return queries\n",
    "    \n",
    "    def hyde(self, query: str):\n",
    "        \n",
    "        prompt = f\"\"\"Write a detailed, factual answer to this question:\n",
    "\n",
    "                Question: {query}\n",
    "\n",
    "                Answer:\"\"\"\n",
    "        \n",
    "        hypothetical_answer = self.llm.invoke(prompt)\n",
    "        return hypothetical_answer\n",
    "    \n",
    "    def step_back(self, query: str):\n",
    "        \n",
    "        prompt = f\"\"\"Given this specific question, generate a broader, more general question:\n",
    "\n",
    "                Specific question: {query}\n",
    "\n",
    "                Broader question:\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96522c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Ollama model: llama3.2\n",
      "Generated queries: ['What is LangChain?', 'What is LangChain a part of?', 'How does LangChain work in the context of blockchain development?', 'Can you provide an overview of the features and capabilities of LangChain?']\n",
      "Hypothetical answer: LangChain is an open-source, decentralized protocol that enables the creation of interoperable, scalable, and secure data storage and sharing networks for blockchain-based applications. It was developed by Lang Technologies, a company founded by Alex Brampton, a well-known figure in the blockchain space.\n",
      "\n",
      "The primary goal of LangChain is to provide a standardized framework for building and connecting different blockchain networks, allowing developers to create seamless, interoperable experiences across various blockchains. This is achieved through the use of a novel data storage and sharing mechanism that enables the creation of decentralized, self-sustaining networks.\n",
      "\n",
      "LangChain's core technology is based on a combination of blockchain-specific concepts, such as smart contracts, tokenomics, and decentralized data storage, with traditional web3 technologies like IPFS (InterPlanetary File System) and Web3.js. This allows LangChain to tap into the strengths of both blockchain-based systems and traditional web3 infrastructure.\n",
      "\n",
      "The protocol consists of several key components:\n",
      "\n",
      "1. **LangChain Network**: A decentralized network that enables the creation of interoperable, scalable data storage and sharing networks.\n",
      "2. **LangChain SDKs**: Software development kits (SDKs) for various programming languages, including Rust, JavaScript, and Python, which provide a standardized interface for building LangChain-based applications.\n",
      "3. **LangChain Wallets**: Customizable wallets that allow users to manage their assets, interact with the LangChain network, and access decentralized data storage services.\n",
      "\n",
      "LangChain's main features include:\n",
      "\n",
      "* **Interoperability**: Enables seamless interactions between different blockchain networks, allowing developers to create cross-chain experiences.\n",
      "* **Scalability**: Supports high-performance data storage and sharing, making it suitable for large-scale applications.\n",
      "* **Security**: Utilizes advanced cryptographic techniques and decentralized data storage mechanisms to ensure the integrity and confidentiality of user data.\n",
      "\n",
      "LangChain has gained significant attention in the blockchain development community due to its innovative approach to interoperability and scalability. Its potential applications include:\n",
      "\n",
      "* **Decentralized data marketplaces**: Enabling secure, transparent, and efficient data sharing across different blockchain networks.\n",
      "* **Cross-chain decentralized finance (DeFi)**: Facilitating seamless interactions between DeFi protocols on various blockchains.\n",
      "* **Decentralized content delivery networks (CDNs)**: Providing a scalable, secure, and high-performance solution for content distribution.\n",
      "\n",
      "Overall, LangChain represents a significant step forward in the development of blockchain-based applications, offering a standardized framework for building interoperable, scalable, and secure data storage and sharing networks.\n",
      "Broader question: Here's a possible broader question:\n",
      "\n",
      "\"What are the key concepts and technologies in blockchain interoperability and decentralized data management?\"\n"
     ]
    }
   ],
   "source": [
    "qt = QueryTransformer(model=\"llama3.2\")  \n",
    "\n",
    "\n",
    "queries = qt.multi_query(\"What is LangChain?\")\n",
    "print(\"Generated queries:\", queries)\n",
    "\n",
    "\n",
    "answer = qt.hyde(\"What is LangChain?\")\n",
    "print(\"Hypothetical answer:\", answer)\n",
    "\n",
    "\n",
    "broader_question = qt.step_back(\"What is LangChain?\")\n",
    "print(\"Broader question:\", broader_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6cf4e",
   "metadata": {},
   "source": [
    "## 7.Context Compression - LLM-based relevance filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46537520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "class ContextCompression:\n",
    "    \n",
    "    def __init__(self,llm):\n",
    "        self.llm=llm\n",
    "        \n",
    "    def compress_documents(self,query:str,documents:List, max_docs=5):\n",
    "        \n",
    "        compressed=[]\n",
    "        \n",
    "        for doc in documents[:max_docs]:\n",
    "            prompt = f\"\"\"Extract only the sentences that directly answer or are relevant to the question below. \n",
    "                    If no sentence is relevant, output \"None\". \n",
    "                    Do not include unrelated information.\n",
    "\n",
    "                    Example:\n",
    "                    Question: What is AI?\n",
    "                    Document: Artificial intelligence (AI) enables machines to learn. Cars have engines.\n",
    "                    Relevant sentences: Artificial intelligence (AI) enables machines to learn.\n",
    "\n",
    "                    Now do the same.\n",
    "\n",
    "                    Question: {query}\n",
    "\n",
    "                    Document: {doc.page_content[:1500]}\n",
    "\n",
    "                    Relevant sentences:\"\"\"\n",
    "            try:\n",
    "                relevant_text=self.llm.invoke(prompt)\n",
    "                if relevant_text.strip():\n",
    "                    compressed.append(Document(\n",
    "                        page_content=relevant_text.strip(),\n",
    "                        metadata=doc.metadata\n",
    "                    ))\n",
    "            except:\n",
    "                compressed.append(doc)\n",
    "        return compressed if compressed else documents[:max_docs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7682bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"RAG stands for Retrieval-Augmented Generation. It combines retrieval and generation. CNN is used for image tasks.\"),\n",
    "    Document(page_content=\"Transformers are used in NLP. RAG helps improve LLM performance by grounding answers.\")\n",
    "]\n",
    "\n",
    "compressor=ContextCompression(llm)\n",
    "result=compressor.compress_documents(\"What is RAG?\",docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f380fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Compressed Documents \n",
      "  RAG stands for Retrieval-Augmented Generation. It combines retrieval and generation. CNN is used for image tasks.\n",
      "\n",
      " Compressed Documents \n",
      "  Transformers are used in NLP. RAG helps improve LLM performance by grounding answers.\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print(\"\\n Compressed Documents \\n \",r.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a8522",
   "metadata": {},
   "source": [
    "## 8.Generation with Citations - Answers with source attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d31d29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class RAGGenerator:\n",
    "    \n",
    "    def __init__(self,model=\"llama3.2\"):\n",
    "        self.llm=Ollama(model=model,temperature=0)\n",
    "        \n",
    "    def generate_with_citations(self,query:str,context_docs:List):\n",
    "        \n",
    "        context_text=\"\"\n",
    "        for i,doc in enumerate(context_docs):\n",
    "            source=doc.metadata.get('source','Unknown')\n",
    "            context_text +=f\"\\n [Source {i+1}]: {source}\\n{doc.page_content}\\n\"\n",
    "            \n",
    "        prompt=f\"\"\"Answer the question based Only on the context provided.\n",
    "        Include citations like [Source X] after each claim.\n",
    "        If the context doesn't contain the answer ,say \"I cannot find this information in the provided source.\"\n",
    "                                                \n",
    "        Context:{context_text}\n",
    "        Question:{query}\n",
    "        Answer with citations: \n",
    "        \"\"\"\n",
    "        answer=self.llm.invoke(prompt)\n",
    "        return answer \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c51109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, RAG (Retrieval-Augmented Generation) stands for Retrieval-Augmented Generation. It combines retrieval and generation models [Source 1].\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"RAG stands for Retrieval-Augmented Generation. It combines retrieval and generation models.\",\n",
    "        metadata={\"source\": \"ai_notes.txt\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Transformers are used in NLP and LLMs.\",\n",
    "        metadata={\"source\": \"ml_intro.txt\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "rag = RAGGenerator(model='llama3.2')\n",
    "\n",
    "query =\"What is RAG?\"\n",
    "answer=rag.generate_with_citations(query,docs)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d13a36",
   "metadata": {},
   "source": [
    "##  9.Evaluation Metrics - MRR, Recall@K, answer quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9313ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_mrr(retrieved_docs,relevant_doc_ids):\n",
    "        for i,doc in enumerate(retrieved_docs):\n",
    "            if doc.metadata.get('doc_id') in relevant_doc_ids:\n",
    "                return 1/(i+1)\n",
    "        return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_recall_at_k(retrieved_docs,relevant_doc_ids,k=5):\n",
    "        retrieved_ids=[doc.metadata.get('doc_id') for doc in retrieved_docs[:k]]\n",
    "        relevant_retrieved=set(retrieved_ids)& set(relevant_doc_ids)\n",
    "        return len(relevant_retrieved)/len(relevant_doc_ids) if relevant_doc_ids else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_precision_at_k(retrieved_docs,relevant_doc_ids,k=5):\n",
    "        retrieved_ids=[doc.metadata.get(\"doc_id\") for doc in retrieved_docs[:k]]\n",
    "        relevant_retrieved=set(retrieved_ids)&set(relevant_doc_ids)\n",
    "        return len(relevant_retrieved)/k if k >0 else 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92e1490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reciprocal rank :  1.0\n",
      "Recall@k :  0.6666666666666666\n",
      "Precision@k :  0.4\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "retrieved_docs = [\n",
    "    Document(page_content=\"About the Eiffel Tower.\", metadata={'doc_id': 'D3'}),\n",
    "    Document(page_content=\"French cuisine and recipes.\", metadata={'doc_id': 'D2'}),\n",
    "    Document(page_content=\"Paris is the capital of France.\", metadata={'doc_id': 'D1'}),\n",
    "    Document(page_content=\"Population of Germany.\", metadata={'doc_id': 'D4'}),\n",
    "    Document(page_content=\"Mountains of Italy.\", metadata={'doc_id': 'D5'}),\n",
    "]\n",
    "\n",
    "relevant_doc_ids = ['D1', 'D3', 'D6']\n",
    "\n",
    "mrr = RAGEvaluator.calculate_mrr(retrieved_docs, relevant_doc_ids)\n",
    "recall = RAGEvaluator.calculate_recall_at_k(retrieved_docs, relevant_doc_ids, k=5)\n",
    "precision =RAGEvaluator.calculate_precision_at_k(retrieved_docs, relevant_doc_ids, k=5)\n",
    "\n",
    "print(\"Mean reciprocal rank : \",mrr)\n",
    "print(\"Recall@k : \",recall)\n",
    "print(\"Precision@k : \",precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a649a3",
   "metadata": {},
   "source": [
    "##    10.Complete Orchestration - Easy-to-use pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ffe45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
