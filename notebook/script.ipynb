{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd89e64b",
   "metadata": {},
   "source": [
    "# Goal\n",
    "    1.Data Ingestion - Load PDFs, text files, HTML, CSVs\n",
    "    2.Advanced Chunking - Recursive, semantic\n",
    "    3.Vector Indexing - ChromaDB\n",
    "    4.Hybrid Search - Dense (embeddings) + Sparse (BM25)\n",
    "    5.Re-ranking - Cohere API & Cross-Encoder models\n",
    "    6.Query Transformation - Multi-query, HyDE, Step-back prompting\n",
    "    7.Context Compression - LLM-based relevance filtering\n",
    "    8.Generation with Citations - Answers with source attribution\n",
    "    9.Evaluation Metrics - MRR, Recall@K, answer quality\n",
    "    10.Complete Orchestration - Easy-to-use pipeline class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db467b2",
   "metadata": {},
   "source": [
    "## 1.Data Ingestion - Load PDFs, text files, HTML, CSVs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630c951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "from langchain.document_loaders import (PyPDFLoader,TextLoader,Docx2txtLoader,DirectoryLoader,UnstructuredHTMLLoader,CSVLoader)\n",
    "from typing import List,Dict,Tuple\n",
    "import re\n",
    "\n",
    "class DataIngestion:\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdfs(file_path:str):\n",
    "        loader=PyPDFLoader(file_path)\n",
    "        return loader.load()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_text(file_path:str):\n",
    "        loader=TextLoader(file_path)\n",
    "        return loader.load()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_directory(directory_path:str,glob_pattern:str='**/*.pdf'):\n",
    "        loader=DirectoryLoader(\n",
    "            directory_path,\n",
    "            glob=glob_pattern,\n",
    "            loader_cls=PyPDFLoader,\n",
    "            show_progress=True\n",
    "        )\n",
    "        return loader.load()\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_docx(file_path:str):\n",
    "        loader=Docx2txtLoader(file_path)\n",
    "        return loader.load()\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_text(text:str)->str:\n",
    "        text=re.sub(r\"\\s+\",' ',text)\n",
    "        text=re.sub(r'[^\\w\\s\\.\\?\\!\\-\\:\\;]','',text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff72b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 7/35 [00:09<00:34,  1.25s/it]Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      " 77%|███████▋  | 27/35 [00:51<00:07,  1.02it/s]Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "100%|██████████| 35/35 [01:07<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# document=DataIngestion.load_directory(r'C:\\Users\\evilk\\OneDrive\\Desktop\\Projects\\RAG-Complete-Pipeline\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37738179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1608\n"
     ]
    }
   ],
   "source": [
    "# print(len(document))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82d2ce",
   "metadata": {},
   "source": [
    "## 2.Advanced Chunking - Recursive, semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from langchain.text_splitter import (RecursiveCharacterTextSplitter,CharacterTextSplitter)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "\n",
    "class Chunking:\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def recursive_chunking(documents,chunk_size=1000,chunk_overlap=200):\n",
    "        textSplitter=RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        return textSplitter.split_documents(documents)\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_chunking(documents,embedding,chunk_size=1000):\n",
    "         chunks=[]\n",
    "         model=SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "         \n",
    "         for doc in documents:\n",
    "             sentences=re.split(r'(?<=[.!?])\\s+',doc.page_content)\n",
    "             \n",
    "             if len(sentences)<=1:\n",
    "                 chunks.append(doc)\n",
    "                 continue\n",
    "             \n",
    "             embedding_array=model.encode(sentences)\n",
    "             \n",
    "             similarities=[]\n",
    "             for i in range(len(embedding_array)-1):\n",
    "                 sim=np.dot(embedding_array[i],embedding_array[i+1])\n",
    "                 similarities.append(sim)\n",
    "                 \n",
    "             threshold=np.percentile(similarities,30)\n",
    "             \n",
    "             current_chunk=[]\n",
    "             for i,sentence in enumerate(sentences):\n",
    "                 current_chunk.append(sentence)\n",
    "                 \n",
    "                 if i <len(similarities) and similarities[i]<threshold:\n",
    "                     chunk_text=' '.join(current_chunk)\n",
    "                     if len(chunk_text)>chunk_size:\n",
    "                         chunks.append(Document(\n",
    "                             page_content=chunk_text,\n",
    "                             metadata=doc.metadata\n",
    "                         ))\n",
    "                         current_chunk=[]\n",
    "                         \n",
    "             if current_chunk:\n",
    "                chunks.append(Document(\n",
    "                    page_content=' '.join(current_chunk),\n",
    "                    metadata=doc.metadata\n",
    "                ))\n",
    "         return chunks\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Salary Risk The present value of the defined plan liability is calculated by reference to the future salaries \n",
      "of plan participants. As such, an increase in the salary of the plan participants will increase the \n",
      "plan's liability.\n",
      "  29.2 Share Based Payments\n",
      "   a) Scheme details\n",
      "    The Company has Employees’ Stock Option Scheme i.e. ESOS-2017 under which options have been granted at the \n",
      "exercise price of C 10 per share to be vested from time to time on the basis of performance and other eligibility criteria. \n",
      "Details of number of options outstanding have been tabulated below: \n",
      "Financial Year\n",
      "(Year of Grant)\n",
      "Number of Options Outstanding\n",
      "Financial Year of Vesting Exercise \n",
      "Price (K)\n",
      "Range of Fair value at Grant \n",
      "Date (K)\n",
      "As at  \n",
      "31st March, \n",
      "2024\n",
      "As at\n",
      "31st March, \n",
      "2023\n",
      "ESOS - 2017\n",
      "Details of Employee Stock Options granted from 1st April, 2020 to 31st March, 2024\n",
      "2020-21 2,00,000 2,00,000 2021-22 to 2024-25 10.00 2,133.40 - 2,151.90' metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 19.5 (Windows)', 'creationdate': '2024-08-22T14:10:09+05:30', 'moddate': '2024-08-22T14:48:11+05:30', 'trapped': '/False', 'source': 'C:\\\\Users\\\\evilk\\\\OneDrive\\\\Desktop\\\\Projects\\\\RAG-Complete-Pipeline\\\\data\\\\Finance policy\\\\standalone.pdf', 'total_pages': 86, 'page': 51, 'page_label': '52'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "chunks=Chunking.recursive_chunking(document)\n",
    "print(chunks[1000])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319cf81",
   "metadata": {},
   "source": [
    "## 3.Vector Indexing - ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import FAISS,Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "class Embeddings:\n",
    "    \n",
    "    def __init__(self,model_name='all-MiniLM-L6-v2'):\n",
    "        \n",
    "        self.embeddings=HuggingFaceEmbeddings(\n",
    "            model_name=f\"sentence-transformers/{model_name}\",\n",
    "            model_kwargs={'device':'cuda'},\n",
    "            encode_kwargs={'normalize_embeddings':True}\n",
    "        )\n",
    "        \n",
    "    def create_chroma_db(self,chunks,persist_directory=\"../chroma_db\"):\n",
    "        vectordb=Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        return vectordb\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evilk\\AppData\\Local\\Temp\\ipykernel_8748\\3264418004.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings=HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "emb=Embeddings()\n",
    "vectordb=emb.create_chroma_db(chunks)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3a676",
   "metadata": {},
   "source": [
    "## 4.Hybrid Search - Dense (embeddings) + Sparse (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc981e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "class HybridRetriever:\n",
    "    \n",
    "    def __init__(self,vectorstore,documents):\n",
    "        self.vectorstore=vectorstore\n",
    "        self.documents=documents\n",
    "        \n",
    "        tokenized_docs=[doc.page_content.lower().split() for doc in documents]\n",
    "        self.bm25=BM25Okapi(tokenized_docs)\n",
    "        print(f\"Hybrid Retriever ready with {len(documents)} documents\")\n",
    "        \n",
    "        \n",
    "    def retrieve(self,query:str,k=10,alpha=0.5):\n",
    "        \n",
    "        # Vector Search \n",
    "        dense_results=self.vectorstore.similarity_search_with_score(query,k=k*2)\n",
    "        \n",
    "        # BM25 Search\n",
    "        tokenized_query=query.lower().split()\n",
    "        bm25_scores=self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        #Normalized Scores between 0-1 \n",
    "        dense_scores=np.array([1/(1+score) for _,score in dense_results])\n",
    "        if dense_scores.max()>dense_scores.min():\n",
    "            dense_scores=(dense_scores-dense_scores.min())/(dense_scores.max()-dense_scores.min())\n",
    "            \n",
    "        if bm25_scores.max()>bm25_scores.min():\n",
    "            bm25_scores=(bm25_scores-bm25_scores.min())/(bm25_scores.max()-bm25_scores.min())\n",
    "            \n",
    "        doc_scores={}\n",
    "        # ADD dense scores\n",
    "        for i, (doc, _) in enumerate(dense_results):\n",
    "            doc_id=id(doc)\n",
    "            doc_scores[doc_id]={'doc':doc,'score':alpha*dense_scores[i]}\n",
    "            \n",
    "        #ADD Sparse scores\n",
    "        for i,doc in enumerate(self.documents):\n",
    "            doc_id=id(doc)\n",
    "            if doc_id in doc_scores:\n",
    "                doc_scores[doc_id]['score']+=(1-alpha)*bm25_scores[i]\n",
    "            else:\n",
    "                doc_scores[doc_id]={'doc':doc,'score':(1-alpha)*bm25_scores[i]}\n",
    "                \n",
    "        # Sort by combined score \n",
    "        sorted_docs=sorted(doc_scores.values(),key=lambda x:x['score'],reverse=True)[:k]\n",
    "        \n",
    "        return[(item['doc'],item['score']) for item in sorted_docs]\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb7520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Retriever ready with 4031 documents\n",
      "0.6   • Absence from place of duty without permission. \n",
      "• Obtaining or attempting to obtain leave or absen\n",
      "0.6   • Absence from place of duty without permission. \n",
      "• Obtaining or attempting to obtain leave or absen\n",
      "0.6   • Absence from place of duty without permission. \n",
      "• Obtaining or attempting to obtain leave or absen\n",
      "0.4   4. TRAVELLING ALLOWANCES \n",
      " \n",
      "4.1 TRANSFER  GRANT \n",
      "Employees will be entitled to one month basic pay p\n",
      "0.3996287450278538   STANDARD OPERATING PROCEDURE – HR                                                                   \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "hybrid = HybridRetriever(vectorstore=vectordb, documents=chunks)\n",
    "\n",
    "query = \"company leave policy for new employees\"\n",
    "results = hybrid.retrieve(query, k=5, alpha=0.6)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(score,\" \",doc.page_content[:100])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c13658",
   "metadata": {},
   "source": [
    "##  5.Re-ranking - Cohere API & Cross-Encoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "class ReRanker:\n",
    "    \n",
    "    def __init__(self,model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        \n",
    "        print(f\"Loading re-ranker model: {model_name}...\")\n",
    "        self.model=CrossEncoder(model_name)\n",
    "        print(\"Re-Ranker loaded\")\n",
    "        \n",
    "    def rerank(self,query:str,documents:List,top_n=5):\n",
    "        \n",
    "        pairs=[[query,doc.page_content] for doc in documents]\n",
    "        \n",
    "        scores=self.model.predict(pairs)\n",
    "        \n",
    "        scored_docs=list(zip(documents,scores))\n",
    "        scored_docs.sort(key=lambda x : x[1],reverse=True)\n",
    "        \n",
    "        return [doc for doc,_ in scored_docs[:top_n]]\n",
    "\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bafc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading re-ranker model: cross-encoder/ms-marco-MiniLM-L-6-v2...\n",
      "Re-Ranker loaded\n",
      "page_content='• Absence from place of duty without permission. \n",
      "• Obtaining or attempting to obtain leave or absence on false \n",
      "pretense. \n",
      "• Refusal to work over time. \n",
      "• Sexual harassment of individuals such as passing of sexual remark \n",
      "and verbal abuse. \n",
      "• Unwelcome physical contact or demand for sexual favors. \n",
      "• Habitual breach of any Standing Orders or any law applicable to the \n",
      "hospital or any rules made hereunder. \n",
      " \n",
      "H. LEAVE POLICY FOR EMPLOYEES \n",
      "To provide and regulate employees' time off from work for personal \n",
      "purposes Metro has put in place a \" Leave Policy\" applicable to \n",
      "Metro employees on the regular rolls of the company. Leave \n",
      "entitlements are provided to enable employees to:- \n",
      "• Rest and recover in case of illness \n",
      "• Attend personal affairs \n",
      "• Take vacations for rest and rejuvenation \n",
      "SL/CL/SPL entitlements coincide with and are determined for the \n",
      "calendar year' January-December \" These would be pro-rated for \n",
      "employees joining or leaving during the year. \n",
      "EL will be generated on the basis of Date of Joining. A weekend, \n",
      "falling in between the leave period, would be counted as a part of \n",
      "leave. Leave records are maintained by the HR Department. \n",
      "CASUAL LEAVE: \n",
      "Casual leave (CL) entitlement is 7 days per annum. Casual leave can \n",
      "be availed after completion of 3 months from date of joining for a' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-01-19T06:42:54+00:00', 'author': 'Microsoft account', 'moddate': '2024-01-19T06:42:58+00:00', 'source': 'C:\\\\Users\\\\evilk\\\\OneDrive\\\\Desktop\\\\Projects\\\\RAG-Complete-Pipeline\\\\data\\\\Hr Policy\\\\HR-POLICIES-1-1-1.pdf', 'total_pages': 47, 'page': 16, 'page_label': '17'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "reranker=ReRanker()\n",
    "top_docs=reranker.rerank(query,document,top_n=5)\n",
    "print(top_docs[0])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5713fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LEAVES- STIPENDARY/ TRAINEE/ INTERNS \n",
      "Staff under this category are not entitled for any hospital/ employee \n",
      "benefits. \n",
      "Procedure For Application of Leave: \n",
      "The employee shall apply leave online & get it approved from his \n",
      "Departmental Head, prior to proceeding on leave, In cases of \n",
      "emergency, leave approval may be taken over the telephone and \n",
      "should be applied online immediately upon return. In such cases it \n",
      "will be the responsibility of the employee to regularize his / her \n",
      "absence. In emergency leave, the HOD will intimate in writing/mail \n",
      "to HR Department regarding the leave of the concerned employee as \n",
      "soon as he receives the information of leave. If any staff is taking \n",
      "leave without prior information, such applications will not be \n",
      "accepted and emplovee will be marked absent from duty for 3 days \n",
      "(1+2). If such leave has been approved, employee's leave records will \n",
      "be updated accordingly. \n",
      "Notes: \n",
      "• Late presentation of Leave will not be accepted. \n",
      "• If an employee is taking leave i.e. CL on Saturday and Monday, then \n",
      "Sunday will not be counted as leave but in case of SPL/Sick leave/EL \n",
      "sunday will be counted as leave. \n",
      "• Visiting Consultants (fee for services)/Stipendary/ Interns/Trainees \n",
      "are not entitled for any type of leaves. \n",
      "•Employee working on Holi and Diwali will be paid extra. \n",
      "All HOD's/Section Incharges should approve leave/ short leave and \n",
      "update roster in 24 hours. Discripency in salary days calculated due \n",
      "to above error will not be considered.' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2024-01-19T06:42:54+00:00', 'author': 'Microsoft account', 'moddate': '2024-01-19T06:42:58+00:00', 'source': 'C:\\\\Users\\\\evilk\\\\OneDrive\\\\Desktop\\\\Projects\\\\RAG-Complete-Pipeline\\\\data\\\\Hr Policy\\\\HR-POLICIES-1-1-1.pdf', 'total_pages': 47, 'page': 19, 'page_label': '20'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "query= \"What’s the leave policy for interns?\"\n",
    "top_docs=reranker.rerank(query,document,top_n=5)\n",
    "print(top_docs[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6244e",
   "metadata": {},
   "source": [
    "## 6.Query Transformation - Multi-query, HyDE, Step-back prompting (API call HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d9a6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "class QueryTransformer:\n",
    "    \n",
    "    def __init__(self,model_name=\"meta-llama/Llama-3.1-8B\"):\n",
    "        \n",
    "        load_dotenv()\n",
    "        hf_token=os.getenv(\"HF_TOKEN\")\n",
    "        \n",
    "        if not hf_token:\n",
    "            raise ValueError(\"HF_TOKEN is not found!!!!\")\n",
    "        \n",
    "        self.client=HuggingFaceEndpoint(\n",
    "            repo_id=model_name,\n",
    "            task=\"text-generation\",\n",
    "            huggingfacehub_api_token=hf_token\n",
    "        )\n",
    "        self.model_name=model_name\n",
    "        print(f\"Using HF model : {model_name}\")\n",
    "        \n",
    "    def _generate(self,prompt:str,max_new_token=256):\n",
    "        \n",
    "        reponse=self.client.invoke(prompt,max_new_tokens=max_new_token)\n",
    "        return reponse.strip()\n",
    "    \n",
    "    def multi_query(self,original_query:str,num_queries=3):\n",
    "        prompt=f\"\"\"Generate {num_queries} different versions of this question to retrieve relevant documents.\n",
    "        Only output the questions, one per line,without numbering.\n",
    "        Original question :{original_query}\n",
    "        Alternative question :\"\"\"\n",
    "        \n",
    "        response = self._generate(prompt)\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "        queries=[original_query]+queries[:num_queries]\n",
    "        return queries\n",
    "    \n",
    "    def hyde(self,query:str):\n",
    "        prompt=f\"\"\"Write a detailed,factual answer to this question:\n",
    "        Question : {query}\n",
    "        Answer :\"\"\"\n",
    "        return self._generate(prompt)\n",
    "    \n",
    "    \n",
    "    def step_back(self,query:str):\n",
    "        prompt=f\"\"\"Give this specific question , generate a broader,more general question:\n",
    "        Specific question :{query}\n",
    "        Broader question :\"\"\"\n",
    "        \n",
    "        return self._generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe980e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HF model : meta-llama/Llama-3.1-8B\n",
      "Multi Queries: ['What is LangChain?', 'What is LangChain?', 'Another alternative question : How does LangChain work?', 'Args:']\n",
      "Hypothetical answer: LangChain is a Python library that simplifies the integration of natural language processing (NLP) models into existing Python applications. It provides a unified interface for working with various NLP models, such as ChatGPT, LLMs, and other APIs. LangChain makes it easy to perform tasks such as text generation, summarization, and question answering, and it allows developers to quickly experiment with different NLP models and tasks.\n",
      "        Overall, LangChain simplifies the process of integrating NLP models into Python applications, making it easier for developers to leverage the power of natural language processing in their projects.\n",
      "    What are some other useful libraries for building NLP applications?\n",
      "    Some other useful libraries for building NLP applications include:\n",
      "    spaCy: A popular library for NLP in Python, providing advanced tokenization, part-of-speech tagging, named entity recognition, and more.\n",
      "    NLTK: A popular library for NLP in Python, providing functions for tokenization, stemming, lemmatization, tagging, parsing, and more.\n",
      "    Scikit-learn: A popular machine learning library in Python, providing functions for data preprocessing, feature engineering, model training, and more.\n",
      "    TensorFlow: A popular machine learning library in Python, providing functions for deep learning\n"
     ]
    }
   ],
   "source": [
    "qt = QueryTransformer(model_name=\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "queries = qt.multi_query(\"What is LangChain?\")\n",
    "print(\"Multi Queries:\", queries)\n",
    "\n",
    "answer = qt.hyde(\"What is LangChain?\")\n",
    "print(\"Hypothetical answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2f778",
   "metadata": {},
   "source": [
    "## 6.Query Transformation - Multi-query, HyDE, Step-back prompting (Local Ollama model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b055587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "class FreeQueryTransformer:\n",
    "    \"\"\"Transform queries using FREE local LLM (Ollama)\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"llama3.2\"):\n",
    "        \"\"\"\n",
    "        Free Ollama models:\n",
    "        - llama3.2: Fast, 3B params (DEFAULT)\n",
    "        - llama3.1: Better, 8B params\n",
    "        - mistral: Alternative, 7B params\n",
    "        - qwen2.5: Very good, 7B params\n",
    "        \"\"\"\n",
    "        self.llm = Ollama(model=model, temperature=0)\n",
    "        print(f\"✅ Using Ollama model: {model}\")\n",
    "    \n",
    "    def multi_query(self, original_query: str, num_queries=3):\n",
    "        \"\"\"Generate multiple query variations - FREE\"\"\"\n",
    "        prompt = f\"\"\"Generate {num_queries} different versions of this question to retrieve relevant documents.\n",
    "Only output the questions, one per line, without numbering.\n",
    "\n",
    "Original question: {original_query}\n",
    "\n",
    "Alternative questions:\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        # Parse response\n",
    "        queries = [q.strip() for q in response.split('\\n') if q.strip() and '?' in q]\n",
    "        queries = [original_query] + queries[:num_queries]\n",
    "        \n",
    "        return queries\n",
    "    \n",
    "    def hyde(self, query: str):\n",
    "        \"\"\"Generate hypothetical answer - FREE\"\"\"\n",
    "        prompt = f\"\"\"Write a detailed, factual answer to this question:\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        hypothetical_answer = self.llm.invoke(prompt)\n",
    "        return hypothetical_answer\n",
    "    \n",
    "    def step_back(self, query: str):\n",
    "        \"\"\"Generate broader question - FREE\"\"\"\n",
    "        prompt = f\"\"\"Given this specific question, generate a broader, more general question:\n",
    "\n",
    "Specific question: {query}\n",
    "\n",
    "Broader question:\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96522c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Ollama model: llama3.2\n",
      "Generated queries: ['What is LangChain?', 'What is LangChain a part of?', 'How does LangChain work in the context of blockchain development?', 'Can you provide an overview of the features and capabilities of LangChain?']\n",
      "Hypothetical answer: LangChain is an open-source, decentralized protocol that enables the creation of interoperable, scalable, and secure data storage and sharing networks for blockchain-based applications. It was developed by Lang Technologies, a company founded by Alex Brampton, a well-known figure in the blockchain space.\n",
      "\n",
      "The primary goal of LangChain is to provide a standardized framework for building and connecting different blockchain networks, allowing developers to create seamless, interoperable experiences across various blockchains. This is achieved through the use of a novel data storage and sharing mechanism that enables the creation of decentralized, self-sustaining networks.\n",
      "\n",
      "LangChain's core technology is based on a combination of blockchain-specific concepts, such as smart contracts, tokenomics, and decentralized data storage, with traditional web3 technologies like IPFS (InterPlanetary File System) and Web3.js. This allows LangChain to tap into the strengths of both blockchain-based systems and traditional web3 infrastructure.\n",
      "\n",
      "The protocol consists of several key components:\n",
      "\n",
      "1. **LangChain Network**: A decentralized network that enables the creation of interoperable, scalable data storage and sharing networks.\n",
      "2. **LangChain SDKs**: Software development kits (SDKs) for various programming languages, including Rust, JavaScript, and Python, which provide a standardized interface for building LangChain-based applications.\n",
      "3. **LangChain Wallets**: Customizable wallets that allow users to manage their assets, interact with the LangChain network, and access decentralized data storage services.\n",
      "\n",
      "LangChain's main features include:\n",
      "\n",
      "* **Interoperability**: Enables seamless interactions between different blockchain networks, allowing developers to create cross-chain experiences.\n",
      "* **Scalability**: Supports high-performance data storage and sharing, making it suitable for large-scale applications.\n",
      "* **Security**: Utilizes advanced cryptographic techniques and decentralized data storage mechanisms to ensure the integrity and confidentiality of user data.\n",
      "\n",
      "LangChain has gained significant attention in the blockchain development community due to its innovative approach to interoperability and scalability. Its potential applications include:\n",
      "\n",
      "* **Decentralized data marketplaces**: Enabling secure, transparent, and efficient data sharing across different blockchain networks.\n",
      "* **Cross-chain decentralized finance (DeFi)**: Facilitating seamless interactions between DeFi protocols on various blockchains.\n",
      "* **Decentralized content delivery networks (CDNs)**: Providing a scalable, secure, and high-performance solution for content distribution.\n",
      "\n",
      "Overall, LangChain represents a significant step forward in the development of blockchain-based applications, offering a standardized framework for building interoperable, scalable, and secure data storage and sharing networks.\n",
      "Broader question: Here's a possible broader question:\n",
      "\n",
      "\"What are the key concepts and technologies in blockchain interoperability and decentralized data management?\"\n"
     ]
    }
   ],
   "source": [
    "qt = FreeQueryTransformer(model=\"llama3.2\")  # choose free model\n",
    "\n",
    "# Multi-query example\n",
    "queries = qt.multi_query(\"What is LangChain?\")\n",
    "print(\"Generated queries:\", queries)\n",
    "\n",
    "# HyDE example\n",
    "answer = qt.hyde(\"What is LangChain?\")\n",
    "print(\"Hypothetical answer:\", answer)\n",
    "\n",
    "# Step-back example\n",
    "broader_question = qt.step_back(\"What is LangChain?\")\n",
    "print(\"Broader question:\", broader_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6cf4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46537520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
